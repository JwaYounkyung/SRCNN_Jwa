{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from datasets import TrainDataset, EvalDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### device setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters setting\n",
    "num_epochs = 4\n",
    "num_classes = 10\n",
    "batch_size = 16\n",
    "\n",
    "kernel_size = [9,1,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset setting\n",
    "train_dataset_file = 'dataset/91-image_x3.h5'\n",
    "eval_dataset_file = 'dataset/Set5_x3.h5'\n",
    "\n",
    "class transform_class:\n",
    "    def __init__(self):\n",
    "        self.transform_crop = None\n",
    "    \n",
    "    def crop(self, fsub1, fsub2):\n",
    "        self.crop_transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(), # HxWxC to CxHxW\n",
    "            torchvision.transforms.CenterCrop((fsub1-sum(kernel_size)+3,fsub2-sum(kernel_size)+3)) # fsub-f1-f2-f3+3 | 33-9-1-5+3\n",
    "        ])\n",
    "        return self.crop_transform\n",
    "\n",
    "tranform_train = transform_class()\n",
    "transform_eval = transform_class()\n",
    "\n",
    "train_dataset = TrainDataset(root=train_dataset_file, target_transform=tranform_train.crop(33,33))\n",
    "eval_dataset = EvalDataset(root=eval_dataset_file, target_transform=transform_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "eval_loader = torch.utils.data.DataLoader(dataset=eval_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train data: (about) 21888\n",
      "number of eval data:  5\n",
      "X_train: torch.Size([16, 1, 33, 33]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16, 1, 21, 21]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 510, 510]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 498, 498]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 288, 288]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 276, 276]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 255, 255]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 243, 243]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 279, 279]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 267, 267]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 342, 228]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 330, 216]) type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "### Checking data\n",
    "print('number of train data: (about)', batch_size*len(train_loader))\n",
    "print('number of eval data: ', 1*len(eval_loader))\n",
    "\n",
    "for X_train, y_train in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type()) # input\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type()) # ground truth high-resolution image\n",
    "    break\n",
    "\n",
    "for X_eval, y_eval in eval_loader:\n",
    "    print('X_eval:', X_eval.size(), 'type:', X_eval.type()) # input\n",
    "    print('y_eval:', y_eval.size(), 'type:', y_eval.type()) # ground truth high-resolution image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Show image\n",
    "fig = plt.figure()\n",
    "rows = 1\n",
    "cols = 2\n",
    "\n",
    "crop_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop(21)\n",
    "])\n",
    "\n",
    "ax1 = fig.add_subplot(rows, cols, 1)\n",
    "img1 = crop_transform(X_train[0][0])\n",
    "ax1.imshow(img1)\n",
    "ax1.set_title('Input data')\n",
    "ax1.axis(\"off\")\n",
    " \n",
    "ax2 = fig.add_subplot(rows, cols, 2)\n",
    "ax2.imshow(y_train[0][0])\n",
    "ax2.set_title('Ground truth')\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SRCNN model\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1): # in_channels : RGB or YCbCr\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_channels, out_channels=64, kernel_size=kernel_size[0], stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=32, kernel_size=kernel_size[1], stride=1, padding=0),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer3 = nn.Conv2d(in_channels=32, out_channels=num_channels, kernel_size=kernel_size[2], stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer3): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1))\n",
      ")\n",
      "\n",
      "\n",
      "Number of parameters: 8129\n",
      "torch.Size([64, 1, 9, 9])\n",
      "torch.Size([64])\n",
      "torch.Size([32, 64, 1, 1])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 32, 5, 5])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "### Configure Optimizer, Objective function\n",
    "model = SRCNN().to(device)\n",
    "optimizer = torch.optim.SGD([\n",
    "                                {'params': model.layer1.parameters()},\n",
    "                                {'params': model.layer2.parameters()},\n",
    "                                {'params': model.layer3.parameters(), 'lr': 1e-5}\n",
    "                            ], lr=1e-4, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def PSNR(img1, img2):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    return 10.0 * torch.log10(1.0 / mse)\n",
    "\n",
    "print(model)\n",
    "print('\\n')\n",
    "print('Number of parameters: {}'.format(sum(p.numel() for p in model.parameters())))\n",
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model training \n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printing the result at the specific interval\n",
    "        if batch_idx % log_interval == 0: # log_interval = 100\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx, len(train_loader), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model evaluating\n",
    "def evaluate(model, eval_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    psnr_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in eval_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(image)\n",
    "\n",
    "            # psnr caculate\n",
    "            psnr_list.append(PSNR(output,label))\n",
    "            eval_loss += criterion(output, label).item()\n",
    "\n",
    "    eval_loss /= len(eval_loader.dataset)\n",
    "    eval_psnr = sum(psnr_list)/len(psnr_list)\n",
    "    return eval_loss, eval_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1368 (0%)]\tTrain Loss: 0.232904\n",
      "Train Epoch: 1 [100/1368 (7%)]\tTrain Loss: 0.078847\n",
      "Train Epoch: 1 [200/1368 (15%)]\tTrain Loss: 0.021153\n",
      "Train Epoch: 1 [300/1368 (22%)]\tTrain Loss: 0.006669\n",
      "Train Epoch: 1 [400/1368 (29%)]\tTrain Loss: 0.011134\n",
      "Train Epoch: 1 [500/1368 (37%)]\tTrain Loss: 0.005458\n",
      "Train Epoch: 1 [600/1368 (44%)]\tTrain Loss: 0.008103\n",
      "Train Epoch: 1 [700/1368 (51%)]\tTrain Loss: 0.006350\n",
      "Train Epoch: 1 [800/1368 (58%)]\tTrain Loss: 0.005433\n",
      "Train Epoch: 1 [900/1368 (66%)]\tTrain Loss: 0.005365\n",
      "Train Epoch: 1 [1000/1368 (73%)]\tTrain Loss: 0.007483\n",
      "Train Epoch: 1 [1100/1368 (80%)]\tTrain Loss: 0.007519\n",
      "Train Epoch: 1 [1200/1368 (88%)]\tTrain Loss: 0.005940\n",
      "Train Epoch: 1 [1300/1368 (95%)]\tTrain Loss: 0.004909\n",
      "\n",
      "[EPOCH: 1/2], \tEvaluate Loss: 0.0059, \tEvaluate PSNR: 23.09 % \n",
      "\n",
      "Train Epoch: 2 [0/1368 (0%)]\tTrain Loss: 0.006956\n",
      "Train Epoch: 2 [100/1368 (7%)]\tTrain Loss: 0.003938\n",
      "Train Epoch: 2 [200/1368 (15%)]\tTrain Loss: 0.008637\n",
      "Train Epoch: 2 [300/1368 (22%)]\tTrain Loss: 0.005133\n",
      "Train Epoch: 2 [400/1368 (29%)]\tTrain Loss: 0.008981\n",
      "Train Epoch: 2 [500/1368 (37%)]\tTrain Loss: 0.005876\n",
      "Train Epoch: 2 [600/1368 (44%)]\tTrain Loss: 0.004013\n",
      "Train Epoch: 2 [700/1368 (51%)]\tTrain Loss: 0.001830\n",
      "Train Epoch: 2 [800/1368 (58%)]\tTrain Loss: 0.002983\n",
      "Train Epoch: 2 [900/1368 (66%)]\tTrain Loss: 0.005175\n",
      "Train Epoch: 2 [1000/1368 (73%)]\tTrain Loss: 0.005367\n",
      "Train Epoch: 2 [1100/1368 (80%)]\tTrain Loss: 0.002465\n",
      "Train Epoch: 2 [1200/1368 (88%)]\tTrain Loss: 0.004346\n",
      "Train Epoch: 2 [1300/1368 (95%)]\tTrain Loss: 0.002517\n",
      "\n",
      "[EPOCH: 2/2], \tEvaluate Loss: 0.0046, \tEvaluate PSNR: 24.61 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Training and Evaluating\n",
    "psnr_epoch = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 100)\n",
    "    eval_loss, eval_psnr = evaluate(model, eval_loader)\n",
    "    print(\"\\n[EPOCH: {}/{}], \\tEvaluate Loss: {:.4f}, \\tEvaluate PSNR: {:.2f} % \\n\".format(\n",
    "        epoch, num_epochs, eval_loss, eval_psnr))\n",
    "    psnr_epoch.append(eval_psnr)\n",
    "\n",
    "pickle.dump(psnr_epoch, open('./results/psnr_list.pickle','wb'))\n",
    "\n",
    "model_weight = copy.deepcopy(model.state_dict())\n",
    "torch.save(model_weight, './results/model_weight.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Graph extract\n",
    "def show_result():\n",
    "    psnr_epoch = pickle.load(open('./results/psnr_list.pickle','rb'))\n",
    "    print(psnr_epoch)\n",
    "    a = [1,2]\n",
    "\n",
    "    plt.plot(list(range(1, len(psnr_epoch)+1)), [for psnr_epoch.cpu())\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('Average PSNR')\n",
    "    plt.savefig('./results/PSNR_Graph.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(25.7963, device='cuda:0'), tensor(26.0250, device='cuda:0'), tensor(26.1681, device='cuda:0'), tensor(26.2658, device='cuda:0')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6de7f662c936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-39b49375d1af>\u001b[0m in \u001b[0;36mshow_result\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsnr_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpsnr_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average PSNR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "show_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31d44ce7392cb46c10d05adfe5c4f9cb38fe608e37917d47628196b44025632f"
  },
  "kernelspec": {
   "display_name": "PyTorch 1.9 (NGC 21.03/Python 3.8 Conda) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
