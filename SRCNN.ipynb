{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from SRCNN_classes import SRCNN, transform_class, PSNR\n",
    "from datasets import TrainDataset, EvalDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### device setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparameters setting\n",
    "num_epochs = 4 # 1000\n",
    "batch_size = 16\n",
    "\n",
    "kernel_size = [9,3,5] # need to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset setting\n",
    "train_dataset_file = 'dataset/91-image_x3.h5'\n",
    "eval_dataset_file = 'dataset/Set5_x3.h5'\n",
    "\n",
    "tranform_train = transform_class(kernel_size=kernel_size)\n",
    "transform_eval = transform_class(kernel_size=kernel_size)\n",
    "\n",
    "train_dataset = TrainDataset(root=train_dataset_file, target_transform=tranform_train.crop(33,33))\n",
    "eval_dataset = EvalDataset(root=eval_dataset_file, target_transform=transform_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "eval_loader = torch.utils.data.DataLoader(dataset=eval_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train data: (about) 21888\n",
      "number of eval data:  5\n",
      "X_train: torch.Size([16, 1, 33, 33]) type: torch.FloatTensor\n",
      "y_train: torch.Size([16, 1, 19, 19]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 510, 510]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 496, 496]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 288, 288]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 274, 274]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 255, 255]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 241, 241]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 279, 279]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 265, 265]) type: torch.FloatTensor\n",
      "X_eval: torch.Size([1, 1, 342, 228]) type: torch.FloatTensor\n",
      "y_eval: torch.Size([1, 1, 328, 214]) type: torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "### Checking data\n",
    "print('number of train data: (about)', batch_size*len(train_loader))\n",
    "print('number of eval data: ', 1*len(eval_loader))\n",
    "\n",
    "for X_train, y_train in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type()) # input\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type()) # ground truth high-resolution image\n",
    "    break\n",
    "\n",
    "for X_eval, y_eval in eval_loader:\n",
    "    print('X_eval:', X_eval.size(), 'type:', X_eval.type()) # input\n",
    "    print('y_eval:', y_eval.size(), 'type:', y_eval.type()) # ground truth high-resolution image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Show image\n",
    "fig = plt.figure()\n",
    "rows = 1\n",
    "cols = 2\n",
    "\n",
    "crop_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop(21)\n",
    "])\n",
    "\n",
    "ax1 = fig.add_subplot(rows, cols, 1)\n",
    "img1 = crop_transform(X_train[1][0])\n",
    "ax1.imshow(img1)\n",
    "ax1.set_title('Input data')\n",
    "ax1.axis(\"off\")\n",
    " \n",
    "ax2 = fig.add_subplot(rows, cols, 2)\n",
    "ax2.imshow(y_train[1][0])\n",
    "ax2.set_title('Ground truth')\n",
    "ax2.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SRCNN(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(9, 9), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (layer3): Conv2d(32, 1, kernel_size=(5, 5), stride=(1, 1))\n",
      ") \n",
      "\n",
      "Number of parameters: 24513\n",
      "torch.Size([64, 1, 9, 9])\n",
      "torch.Size([64])\n",
      "torch.Size([32, 64, 3, 3])\n",
      "torch.Size([32])\n",
      "torch.Size([1, 32, 5, 5])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "### Configure Optimizer, Objective function\n",
    "model = SRCNN(kernel_size=kernel_size).to(device)\n",
    "optimizer = torch.optim.SGD([\n",
    "                                {'params': model.layer1.parameters()},\n",
    "                                {'params': model.layer2.parameters()},\n",
    "                                {'params': model.layer3.parameters(), 'lr': 1e-5}\n",
    "                            ], lr=1e-4, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model, '\\n')\n",
    "print('Number of parameters: {}'.format(sum(p.numel() for p in model.parameters())))\n",
    "for p in model.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model training \n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Printing the result at the specific interval\n",
    "        if batch_idx % log_interval == 0: # log_interval = 100\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx, len(train_loader), 100. * batch_idx / len(train_loader),\n",
    "                loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### model evaluation\n",
    "def evaluate(model, eval_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    psnr_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in eval_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = model(image)\n",
    "\n",
    "            # psnr caculate\n",
    "            psnr_list.append(PSNR(output,label))\n",
    "            eval_loss += criterion(output, label).item()\n",
    "\n",
    "    eval_loss /= len(eval_loader.dataset)\n",
    "    eval_psnr = sum(psnr_list)/len(psnr_list)\n",
    "    return eval_loss, eval_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1368 (0%)]\tTrain Loss: 0.003898\n",
      "Train Epoch: 1 [100/1368 (7%)]\tTrain Loss: 0.006416\n",
      "Train Epoch: 1 [200/1368 (15%)]\tTrain Loss: 0.003439\n",
      "Train Epoch: 1 [300/1368 (22%)]\tTrain Loss: 0.005138\n",
      "Train Epoch: 1 [400/1368 (29%)]\tTrain Loss: 0.004284\n",
      "Train Epoch: 1 [500/1368 (37%)]\tTrain Loss: 0.005160\n",
      "Train Epoch: 1 [600/1368 (44%)]\tTrain Loss: 0.004120\n",
      "Train Epoch: 1 [700/1368 (51%)]\tTrain Loss: 0.005202\n",
      "Train Epoch: 1 [800/1368 (58%)]\tTrain Loss: 0.004686\n",
      "Train Epoch: 1 [900/1368 (66%)]\tTrain Loss: 0.004645\n",
      "Train Epoch: 1 [1000/1368 (73%)]\tTrain Loss: 0.003754\n",
      "Train Epoch: 1 [1100/1368 (80%)]\tTrain Loss: 0.004066\n",
      "Train Epoch: 1 [1200/1368 (88%)]\tTrain Loss: 0.005384\n",
      "Train Epoch: 1 [1300/1368 (95%)]\tTrain Loss: 0.004497\n",
      "\n",
      "[EPOCH: 1/4], \tEvaluate Loss: 0.0046, \tEvaluate PSNR: 25.11 % \n",
      "\n",
      "Train Epoch: 2 [0/1368 (0%)]\tTrain Loss: 0.007826\n",
      "Train Epoch: 2 [100/1368 (7%)]\tTrain Loss: 0.004110\n",
      "Train Epoch: 2 [200/1368 (15%)]\tTrain Loss: 0.003371\n",
      "Train Epoch: 2 [300/1368 (22%)]\tTrain Loss: 0.004808\n",
      "Train Epoch: 2 [400/1368 (29%)]\tTrain Loss: 0.002240\n",
      "Train Epoch: 2 [500/1368 (37%)]\tTrain Loss: 0.004584\n",
      "Train Epoch: 2 [600/1368 (44%)]\tTrain Loss: 0.004846\n",
      "Train Epoch: 2 [700/1368 (51%)]\tTrain Loss: 0.002598\n",
      "Train Epoch: 2 [800/1368 (58%)]\tTrain Loss: 0.004263\n",
      "Train Epoch: 2 [900/1368 (66%)]\tTrain Loss: 0.004031\n",
      "Train Epoch: 2 [1000/1368 (73%)]\tTrain Loss: 0.003115\n",
      "Train Epoch: 2 [1100/1368 (80%)]\tTrain Loss: 0.003744\n",
      "Train Epoch: 2 [1200/1368 (88%)]\tTrain Loss: 0.004737\n",
      "Train Epoch: 2 [1300/1368 (95%)]\tTrain Loss: 0.003180\n",
      "\n",
      "[EPOCH: 2/4], \tEvaluate Loss: 0.0042, \tEvaluate PSNR: 25.59 % \n",
      "\n",
      "Train Epoch: 3 [0/1368 (0%)]\tTrain Loss: 0.004354\n",
      "Train Epoch: 3 [100/1368 (7%)]\tTrain Loss: 0.003430\n",
      "Train Epoch: 3 [200/1368 (15%)]\tTrain Loss: 0.006388\n",
      "Train Epoch: 3 [300/1368 (22%)]\tTrain Loss: 0.005026\n",
      "Train Epoch: 3 [400/1368 (29%)]\tTrain Loss: 0.003577\n",
      "Train Epoch: 3 [500/1368 (37%)]\tTrain Loss: 0.004071\n",
      "Train Epoch: 3 [600/1368 (44%)]\tTrain Loss: 0.003012\n",
      "Train Epoch: 3 [700/1368 (51%)]\tTrain Loss: 0.006344\n",
      "Train Epoch: 3 [800/1368 (58%)]\tTrain Loss: 0.002091\n",
      "Train Epoch: 3 [900/1368 (66%)]\tTrain Loss: 0.004321\n",
      "Train Epoch: 3 [1000/1368 (73%)]\tTrain Loss: 0.004481\n",
      "Train Epoch: 3 [1100/1368 (80%)]\tTrain Loss: 0.002136\n",
      "Train Epoch: 3 [1200/1368 (88%)]\tTrain Loss: 0.003405\n",
      "Train Epoch: 3 [1300/1368 (95%)]\tTrain Loss: 0.004630\n",
      "\n",
      "[EPOCH: 3/4], \tEvaluate Loss: 0.0039, \tEvaluate PSNR: 25.98 % \n",
      "\n",
      "Train Epoch: 4 [0/1368 (0%)]\tTrain Loss: 0.005799\n",
      "Train Epoch: 4 [100/1368 (7%)]\tTrain Loss: 0.002585\n",
      "Train Epoch: 4 [200/1368 (15%)]\tTrain Loss: 0.005166\n",
      "Train Epoch: 4 [300/1368 (22%)]\tTrain Loss: 0.003225\n",
      "Train Epoch: 4 [400/1368 (29%)]\tTrain Loss: 0.004700\n",
      "Train Epoch: 4 [500/1368 (37%)]\tTrain Loss: 0.004121\n",
      "Train Epoch: 4 [600/1368 (44%)]\tTrain Loss: 0.003496\n",
      "Train Epoch: 4 [700/1368 (51%)]\tTrain Loss: 0.003676\n",
      "Train Epoch: 4 [800/1368 (58%)]\tTrain Loss: 0.003633\n",
      "Train Epoch: 4 [900/1368 (66%)]\tTrain Loss: 0.002991\n",
      "Train Epoch: 4 [1000/1368 (73%)]\tTrain Loss: 0.002240\n",
      "Train Epoch: 4 [1100/1368 (80%)]\tTrain Loss: 0.001440\n",
      "Train Epoch: 4 [1200/1368 (88%)]\tTrain Loss: 0.002922\n",
      "Train Epoch: 4 [1300/1368 (95%)]\tTrain Loss: 0.002810\n",
      "\n",
      "[EPOCH: 4/4], \tEvaluate Loss: 0.0036, \tEvaluate PSNR: 26.32 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Training and Evaluation\n",
    "psnr_epoch = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 100)\n",
    "    eval_loss, eval_psnr = evaluate(model, eval_loader)\n",
    "    print(\"\\n[EPOCH: {}/{}], \\tEvaluate Loss: {:.4f}, \\tEvaluate PSNR: {:.2f} % \\n\".format(\n",
    "        epoch, num_epochs, eval_loss, eval_psnr))\n",
    "    psnr_epoch.append(eval_psnr)\n",
    "\n",
    "pickle.dump(psnr_epoch, open('./results/psnr_list.pickle','wb'))\n",
    "\n",
    "model_weight = copy.deepcopy(model.state_dict())\n",
    "torch.save(model_weight, './results/model_weight.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31d44ce7392cb46c10d05adfe5c4f9cb38fe608e37917d47628196b44025632f"
  },
  "kernelspec": {
   "display_name": "PyTorch 1.9 (NGC 21.03/Python 3.8 Conda) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
